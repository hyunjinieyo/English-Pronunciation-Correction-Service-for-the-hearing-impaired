# -*- coding: utf-8 -*-
"""recognition_scoring_공유용.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/127dhEAQetImv8_cC9jxJJKne7qLlycQL

# 1. package 설치
"""

! pip install librosa
! pip install pydub
!pip install noisereduce
!pip install transformers
!pip install phonemizer
!pip install dtw-python

!pip install py-espeak-ng
!sudo apt-get install python-espeak
!sudo apt-get update && sudo apt-get install espeak

# 구글 드라이브 mount
from google.colab import drive
drive.mount('/content/drive')

"""# 2. 파일 확장자 변환 및 전처리"""

# 전처리 코드
from scipy.io import wavfile
import noisereduce as nr
import IPython
import numpy as np
import matplotlib.pyplot as plt
from pydub import AudioSegment

# wav파일을 read하면서 ndarray가 2차원을 가지게 되는 경우도 생김
## reduced_noise에 넣어 작업하려면 무조건 1차원으로 들어가야 함
## 따라서 2차원 data가 생성되었을 때, 1차원으로 바꾸어주는 작업을 하는 메소드 생성

class DimentionOverflow(Exception):
    def __init__(self):
      pass

    def __str__(self):
      return "The dimention is greater than 2"
      
# 2차원 => 1차원으로 바꿔주는 함수
def convert_dim(data):
  assert type(data) == np.ndarray, "Data type is wrong"
  
  if data.ndim == 1:
    return data
  
  elif data.ndim == 2:
    return data[:, 0]
  
  else:
    raise DimentionOverflow

# wav 변환 method
def to_wav(file_path, file_name):
    audSeg = AudioSegment.from_file(file_path)
    audSeg.export(f"{file_name}.wav", format="wav")
    print(f'{file_name}.wav convert success!')

# noise 제거 작업
def reduce_noise(file_path, file_name):
    rate, data = wavfile.read(file_path)
    data_r = convert_dim(data)

    reduced_noise = nr.reduce_noise(y=data_r, sr=rate)
    wavfile.write(f"{file_name}_reducednoise.wav", rate, reduced_noise)

from pydub import AudioSegment

# 파일 불러오기
native = "/content/drive/MyDrive/Vinsenjo/Sun/data/voice/voice_ryan.mp3"
speaker = "/content/drive/MyDrive/Vinsenjo/Sun/data/voice/Speak.m4a"

# wav 변환                                                     
to_wav(native, 'voice_ryan')
to_wav(speaker, 'speak')

# 데이터 로드
path = "/content/speak.wav"

# 발화자의 파일(speak.wav)의 noise reduce
reduce_noise(path, "speak")

"""# 3. STT 및 Text -> phoneme 변환"""

# from transformers.models.wav2vec2_phoneme import tokenization_wav2vec2_phoneme
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer
from phonemizer import phonemize
import torch
import librosa

# 음원 -> 음소
def speak_to_phoneme(audio, tokenizer, model, is_stress=False):
    assert type(audio) == np.ndarray
    # 유저 발화 파일 tokenizer에 넣기
    input_values = tokenizer(audio, return_tensors = "pt").input_values

    # 모델을 통해 logit값 출력(non_normalized)
    logits = model(input_values).logits

    # argmax를 통해 가장 가능성 높은 logits 들을 예측 logits으로
    prediction = torch.argmax(logits, dim = -1)

    # decoeding해서 text로 변환
    transcription = tokenizer.batch_decode(prediction)[0]

    phoneme = text_to_phoneme(transcription, is_stress)
    return transcription, phoneme


# text -> 음소
def text_to_phoneme(transcription, is_stress=False):
    assert type(transcription) == str
    # 라이브러리를 활용해서 phoneme 변환
    phoneme = phonemize(transcription, with_stress=is_stress).rstrip()
    return phoneme



# model과 tokenizer pre-trained된 것 가져오기

tokenizer = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

# librosa를 통해 load
ans, rate1 = librosa.load("/content/voice_ryan.wav", sr = 16000)
deaf, rate2 = librosa.load("/content/speak.wav", sr = 16000)

# 음소 변환 작업

ans_transcription = "Hello, my name is Ryan"
ans_phoneme = text_to_phoneme(ans_transcription, is_stress=False)

print('정답')
print(ans_transcription)
print(ans_phoneme, end='\n\n')


deaf_transcription, deaf_phoneme = speak_to_phoneme(deaf, tokenizer, model, is_stress=False)

print('발화자')
print(deaf_transcription)
print(deaf_phoneme)

# # 시퀀스 `X[0…m-1]` 및 `Y[0…n-1]`의 가장 긴 공통 부분 문자열을 찾는 함수
# def LCS(X, Y, m, n):
 
#     maxLength = 0           #는 LCS의 최대 길이를 저장합니다.
#     endingIndex = m         #는 `X`에 LCS의 끝 인덱스를 저장합니다.
 
#     # `lookup[i][j]`는 하위 문자열 `X[0…i-1]` 및 `Y[0…j-1]`의 LCS 길이를 저장합니다.
#     lookup = [[0 for x in range(n + 1)] for y in range(m + 1)]
 
#     # 상향식 방식으로 조회 테이블 채우기
#     for i in range(1, m + 1):
#         for j in range(1, n + 1):
 
#             # `X`와 `Y`의 현재 문자가 일치하는 경우
#             if X[i - 1] == Y[j - 1]:
#                 lookup[i][j] = lookup[i - 1][j - 1] + 1
 
#                 #는 최대 길이와 끝 인덱스를 업데이트합니다.
#                 if lookup[i][j] > maxLength:
#                     maxLength = lookup[i][j]
#                     endingIndex = i

#     #는 길이가 `maxLength`인 가장 긴 공통 부분 문자열을 반환합니다.
#     return X[endingIndex - maxLength: endingIndex]
 


# X = ans_phonemes
# Y = deaf_phonemes

# m = len(X)
# n = len(Y)

# # 가장 긴 공통 부분 문자열 찾기
# print('The longest common substring is', LCS(X, Y, m, n))

# LCS 알고리즘을 사용하여 두 string 중에서 겹치는 음소 subsequence 출력 용도
def lcs_algo(S1, S2, m, n):
    L = [[0 for x in range(n+1)] for x in range(m+1)]

    # bottom-up 방식으로 matrix 쌇아감
    for i in range(m+1):
        for j in range(n+1):
            if i == 0 or j == 0:
                L[i][j] = 0
            elif S1[i-1] == S2[j-1]:
                L[i][j] = L[i-1][j-1] + 1
            else:
                L[i][j] = max(L[i-1][j], L[i][j-1])

    index = L[m][n]

    lcs_algo = [""] * (index+1)
    lcs_algo[index] = ""

    i = m
    j = n
    while i > 0 and j > 0:

        if S1[i-1] == S2[j-1]:
            lcs_algo[index-1] = S1[i-1]
            i -= 1
            j -= 1
            index -= 1

        elif L[i-1][j] > L[i][j-1]:
            i -= 1
        else:
            j -= 1
            
    #  subsequences 출력
    print("S1 : " + S1 + "\nS2 : " + S2)

    lcs = "".join(lcs_algo)
    print("LCS: " + lcs)

    return lcs

# 정확도 및 score 반환
def calculate_acc(ans, lcs):
    accuracy = int(len(lcs) / len(ans) * 100)
    score = ""

    # 일단 단계는 임의로 둠
    if accuracy == 100:
        score = "Perfect"
    elif accuracy >= 80:
        score = "Great"
    elif accuracy >= 60:
        score == "Good"
    else:
        score == "Try Again"

    return accuracy, score

# 정답 음소와 발화자의 음소 중, 일치하는 음소 구분해서 출력해주는 함수
def highlight(ans, lcs):
    
  # answer phonemes중에 틀린것은 0, 일치하는 것은 1로 두어, 0인것은 나중에 틀린것 표시하기 위함
  correct = [[i, 0] for i in ans]
  idx=0

  # lcs에서 하니씩 음소를 가져와서 answer과 비교함
  ## 일치하면 correct의 해당 음소의 값을 0에서 1로
  for char in lcs:
      while idx < len(ans):
          tmp = ans[idx]

          if tmp == char:
              correct[idx][1] = 1
              idx += 1
              break
          idx += 1
    
  return correct

S1 = ans_phoneme
S2 = deaf_phoneme
m = len(S1)
n = len(S2)
lcs = lcs_algo(S1, S2, m, n)

print(S1)
print(lcs)

highlight(S1, lcs)

calculate_acc(S1, lcs)



"""# wave -> energy추출해서 rms함 (mel_spectrogram 활용하는 방법은 미완료)
- 다음과 같이 pitch를 추출할 수 있지 않을까??
"""

# rms 값 추출
rms1 = librosa.feature.rms(ans)
rms2 = librosa.feature.rms(deaf)

print(rms1)
print(rms2)
print(rms1.shape, rms2.shape)

import matplotlib.pyplot as plt
fig, ax = plt.subplots(nrows=2, sharex=True)
times = librosa.times_like(rms1)
ax[0].semilogy(times, rms1[0], label='RMS Energy')
ax[0].set(xticks=[])
ax[0].legend()
ax[0].label_outer()

times = librosa.times_like(rms2)
ax[1].semilogy(times, rms2[0], label='RMS Energy')
ax[1].set(xticks=[])
ax[1].legend()
ax[1].label_outer()
# librosa.specshow(librosa.amplitude_to_db(S, ref=np.max),
#                          y_axis='log', x_axis='time', ax=ax[1])
# ax[1].set(title='log Power spectrogram')

import dtw
dtw.dtw(rms1.flatten(), rms2.flatten(), keep_internals=True).plot(type='twoway')

alignment = dtw.dtw(rms1.flatten(), rms2.flatten(), keep_internals=True)
alignment

def downsample(a, b):
    a, b = a.flatten(), b.flatten()
    if len(a) > len(b):
        target, compare = a, b
    else:
        target, compare = b, a

    sampled_list = []
    window_size = len(target) - len(compare) + 1
    for i in range(len(compare)):
        tmp = np.mean(target[i:i+window_size])
        sampled_list.append(tmp)
        
    return np.array(sampled_list)

import math

# 두 개의 다른 signal 길이 맞춰주는 함수 고안
def downsample(a, b):
    a, b = a.flatten(), b.flatten()
    if len(a) > len(b):
        target, compare = a, b
    else:
        target, compare = b, a

  # 정규화
    norm = np.linalg.norm(target)
    target = target/norm

    norm = np.linalg.norm(compare)
    compare = compare/norm

    sampled_list = []
    padding_size = int(len(target) / len(compare))
    print(padding_size)
    if padding_size > 1:
        for i in range(0, len(target), padding_size):
            tmp = np.mean( target[i:i+padding_size] )
            sampled_list.append(tmp)
    else:
        per = np.percentile(target, 100 - len(compare)/len(target)*100)
        print('percentile')
        print(len(compare)/len(target))
        print(per)
        sampled_list = target[(target > per)]

    return compare, np.array(sampled_list)

a = np.array([1,2,3,4,5,6,7,8, 9, 10])
b = np.array([1,2,3,4])

new_rms1, new_rms2 = downsample(rms1, rms2)
new_rms2.shape

new_alignment = dtw.dtw(new_rms1.flatten(), new_rms2.flatten(), keep_internals=True)
new_alignment.plot(type='twoway')

new_alignment.normalizedDistance

new_rms2

